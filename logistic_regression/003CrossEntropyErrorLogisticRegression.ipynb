{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closed form solution of logistic regression is not found generally and we use gradient descent to find the solution.\n",
    "\n",
    "# What is closed form solution anyway?\n",
    "\n",
    "# In our example, closed form will be possible as we have two gaussian distributions with same covariance and different mean.\n",
    "\n",
    "# What is multivariate gaussian distribution?\n",
    "\n",
    "# P(Y|X) = (P(X|Y) * P(Y)) / P(X)\n",
    "\n",
    "# P(Y|X) - posterior,   P(X|Y) - likelihood, P(Y) - prior,   \n",
    "\n",
    "# Now, if we manipulate baye's rule a little bit for P(Y = 1| X), we get \n",
    "# p(y=1|x) = 1/(1 + ( (p(x|y=0)*p(y=0))/(p)(x|y=1)*p(y=1) ))\n",
    "\n",
    "# looks a lot like logistic regression's sigmoid function, if we compare them we will find\n",
    "\n",
    "# -(w.T.x + b)  = ln( (p(x|y=0)*p(y=0))/(p)(x|y=1)*p(y=1) )\n",
    "\n",
    "# we can solve it \n",
    "\n",
    "# Covariance and its inverse is symmetric. \n",
    "\n",
    "# If covariance is diagonal matrix then its a case of Naive Bayes\n",
    "\n",
    "# Above method is called LDA since we have same covariances and if we have different covariances then it would be QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Error - Squared Error\n",
    "# J = (t - y)^2\n",
    "# squared error can't be used for logistic regression as it is assumed to be gaussian distributed.\n",
    "# Mean squared error actually is not taken as random but when errors are normally distributed then cost function comes \n",
    "# as mean sqyared error, this is proven \n",
    "\n",
    "\n",
    "# But logistic regression error can't be gaussian distributed as target is 0 or 1 and y is a number between 0 and 1.\n",
    "\n",
    "# So for logistic regression we get something called cross entropy error or log loss. \n",
    "# This error function is 0 when there is no error and when there is an error, in that case bigger is the cost of more \n",
    "# wrong is the prediction\n",
    "\n",
    "# J = - ( tlogy + (1-t)log(1-y) ), where t is target (0 or 1) and y = output of logistic function.\n",
    "\n",
    "# At one point of time, only one term will be useful \n",
    "# we can check for the different values of t and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "D = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279.32003320442317\n"
     ]
    }
   ],
   "source": [
    "# Creating data\n",
    "X = np.random.randn(N, D)\n",
    "\n",
    "# center the first 50 points at -2, -2\n",
    "X[:50, :] = X[:50, :] - 2*np.ones((50, D))\n",
    "\n",
    "# center the last 50 points at 2, 2\n",
    "X[50:, :] = X[50:, :] + 2*np.ones((50, D))\n",
    "\n",
    "# Creating target values\n",
    "T = np.array([0]*50 + [1]*50)\n",
    "\n",
    "\n",
    "# Now add the bias term here in X only \n",
    "ones = np.ones((N, 1))\n",
    "Xb = np.concatenate((ones, X), axis=1)\n",
    "\n",
    "# randomly initialize weights \n",
    "w = np.random.randn(D+1)\n",
    "\n",
    "# calculate the model output \n",
    "z = Xb.dot(w)\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "\n",
    "\n",
    "y = sigmoid(z)\n",
    "\n",
    "\n",
    "# calculate the cross entropy error\n",
    "def cross_entropy(T, y):\n",
    "    E = 0\n",
    "    for i in range(len(T)):\n",
    "        # summing up all the error function\n",
    "        if T[i] == 1:\n",
    "            E -= np.log(y[i])\n",
    "        else:\n",
    "            E -= np.log(1 - y[i])\n",
    "            \n",
    "    return E\n",
    "\n",
    "# calculate the cross entropy error\n",
    "print(cross_entropy(T, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08313117987668509\n"
     ]
    }
   ],
   "source": [
    "# Let's try with our closed form solution \n",
    "w = np.array([0, 4, 4])\n",
    "\n",
    "# calculate the model output \n",
    "z = Xb.dot(w)\n",
    "y = sigmoid(z)\n",
    "\n",
    "\n",
    "# calculate the cross entropy error\n",
    "print(cross_entropy(T, y))\n",
    "\n",
    "# Cross entropy error in this case is very less as we used closed form solution weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
